{
    "componentChunkName": "component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx",
    "path": "/audio-synthesis-in-tts-system",
    "result": {"data":{"post":{"slug":"/audio-synthesis-in-tts-system","title":"Audio Synthesis in TTS System","date":"14.02.2023","tags":[{"name":"NLP","slug":"nlp"},{"name":"TTS","slug":"tts"}],"description":null,"canonicalUrl":null,"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"slug\": \"audio-synthesis-in-tts-system\",\n  \"title\": \"Audio Synthesis in TTS System\",\n  \"date\": \"2023-02-14T00:00:00.000Z\",\n  \"tags\": [\"NLP\", \"TTS\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"In theory, we can just convert a spectrogram to audio by using inverse FFT(iFFT). However, there are two most challenging problems:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"The mel spectrogram does not contain the \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"phrase information\"), \" output by the original FFT.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"The predicted spectrogram is \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"imperfect\"), \". It is likely smoother than the GT and may contain noise or other \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"unnatural characteristics/artifacts.\"))), mdx(\"p\", null, \"In problem one, we could approximate the phrase using the \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Griffin-Lim algorithm.\")), mdx(\"p\", null, \"In problem two, we train a separate model called a \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"vocoder\"), \" to generate the audio. This model learned to do the audio reconstruction more accurately. \"), mdx(\"h2\", null, \"Spectrogram Inversion Modeling (Vocoder):\"), mdx(\"h3\", null, \"1. WaveNet: a very large auto-regressive dilated causal CNN\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1609.03499\"\n  }, \"WaveNet\"), \" was developed by researchers at DeepMind in 2016, The architecture of WaveNet is based on CNNs. One of the key innovations of WaveNet is its use of dilated causal CNNs, which allow the network to have a large \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"receptive field\"), \" while still maintaining a small number of parameters.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"960px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"56.25%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABuElEQVQoz0WOW4/TMBCF80+Wl27T/v8/wMMKBMtlBQLxgCip4+v4FtuJPXZWKO2yPToPRzPz6UzXkkT6gPILqicUj4W8RfEJ1Vdk77Ygn1B83rbsPfIPyD+i+lboA4rHttguZ8zYCq6lXv38ErZhK3XFq9vz5kso2HJpOZeulFKWFI0IRkYH0aotGBUtRAfBqgmEldQp7oBbybRSy7ystZWSNzjnDAAawGitQWulOeVkGNnIKWFs5JJLwSQlbDgNlFCrbdvga3MpIYQYo/f+AhslgXORQopTnFww2mow1noAPZzJSOgNRsSUknOOMRZCTDEBgFIq5zz5iVLqnJnnxXAlToSR0WjT2n+4tXa5cJxz773gHACkENZYKZXYho6REf4Qdjqfvv/iUr7COc55EFbYJFwUfmYmjnpiNggbmQ5EeWoile788/z3x8B+D3Vttdacc4eIFbGtK85L1lOxU/GxTKnYUHzEZfur1rpVhVR8rAXXda24qTNXWaulUieizlRRDkzAyIAKrcDY1xNrnQOl6Dhq0MaYrr/p0B+P/fHQHy6+hv5w2+/3/b7f7XZv7u729/f9vv8H1QE7APWnjg4AAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Untitled\",\n    \"title\": \"Untitled\",\n    \"src\": \"/static/8d22b0e7ed3a3e32f0bd3fdc325fe402/7d769/Untitled.png\",\n    \"srcSet\": [\"/static/8d22b0e7ed3a3e32f0bd3fdc325fe402/5243c/Untitled.png 240w\", \"/static/8d22b0e7ed3a3e32f0bd3fdc325fe402/ab158/Untitled.png 480w\", \"/static/8d22b0e7ed3a3e32f0bd3fdc325fe402/7d769/Untitled.png 960w\", \"/static/8d22b0e7ed3a3e32f0bd3fdc325fe402/f3baa/Untitled.png 1280w\"],\n    \"sizes\": \"(max-width: 960px) 100vw, 960px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(\"p\", null, \"WaveNet has been used in a variety of applications, including TTS system, music generation, and noise reduction. It has also been integrated into Google\\u2019s Assistant to improve the quality of speech synthesis\"), mdx(\"h3\", null, \"2. HiFi-GAN:\"), mdx(\"p\", null, \"HiFi-GAN was created by a team of researchers from Tencent AI Lab in 2020. Their work was published in a research paper titled \\u201C\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/pdf/2010.05646.pdf\"\n  }, \"HiFi-GAN: High-Fidelity Generative Adversarial Networks for Audio Synthesis\"), \"\\u201D. \"), mdx(\"p\", null, \"The architecture is similar to WaveNet but it\\u2019s smaller and not auto-regressive \\u21D2 10000x faster than WaveNet. HiFi-GAN is trained through an adversarial training method, treating the vocoder like a generative adversarial network (GAN). It uses \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"multiple scale discriminators\"), \", which are temporal CNNs that try to classify the audio as real or fake after average pooling sets of adjacent audio samples, and \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"period discriminators\"), \" which try to classify using audio sampled over different periods\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"813px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"44.583333333333336%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABJ0AAASdAHeZh94AAABHUlEQVQoz4WSa26DMBCEuf/V6AXKj5BUlaAYx+/YYJuJvJVdBUWqpQUDn2bHO3Q4gOM4/q2ycs5tX1d9rvfuDLxblRF3jvlngrYaSglYZ9r3JlguVgfI1UFyB7E6OBPITYwRKaUGG2OolmUBYwvudw5rLbFVmASdclBMQq8KahF46AeUUpjnGVLK5nS8DODLN4xicIbDGgEhBDjn1Kg5TNEj7hZb0Ii7wb4/oLWm8t43wfLeewnnBEJQCN5gXVdyXBuTYIwJl8uIvv/A7faFnN8PPqWM6/WGvu8xjlcax5nranql0zB8gjH2Muhzir/cQFxN/SWUuilHm6YJ27a9CJxdhhCIq6M4c93fcRJVSa2ke/7HGpczOSshhDfNnwqfvtBipzBsAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Untitled\",\n    \"title\": \"Untitled\",\n    \"src\": \"/static/370b8a763c8532086e5d56346b1b9178/44eb4/Untitled1.png\",\n    \"srcSet\": [\"/static/370b8a763c8532086e5d56346b1b9178/5243c/Untitled1.png 240w\", \"/static/370b8a763c8532086e5d56346b1b9178/ab158/Untitled1.png 480w\", \"/static/370b8a763c8532086e5d56346b1b9178/44eb4/Untitled1.png 813w\"],\n    \"sizes\": \"(max-width: 813px) 100vw, 813px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(\"h3\", null, \"3. WaveRNN:\"), mdx(\"p\", null, \"WaveRNN is an autoregressive model that uses a Recurrent Neural Network (RNN) architecture to generate audio samples one sample at a time. \"), mdx(\"p\", null, \"Since the model uses RNNs, training time requires a long time compared to CNNs and Transformers. However one of the advantages of WaveRNN is its ability to generate high-quality audio in real-time, making it useful for real-world applications and industries. Especially, \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"this model is an ideal vocoder for low-latency and low-power situations like on-device TTS\"), \" due to their small compute requirements and memory footprint at inference time.\"));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"In theory, we can just convert a spectrogram to audio by using inverse FFT(iFFT). However, there are two most challenging problems: The melâ€¦","timeToRead":1,"banner":null}},"pageContext":{"slug":"/audio-synthesis-in-tts-system","formatString":"DD.MM.YYYY"}},
    "staticQueryHashes": ["2744905544","3090400250","318001574"]}