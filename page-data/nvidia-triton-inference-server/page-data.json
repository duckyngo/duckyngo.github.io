{
    "componentChunkName": "component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx",
    "path": "/nvidia-triton-inference-server",
    "result": {"data":{"post":{"slug":"/nvidia-triton-inference-server","title":"Technical Notes: NVIDIA Triton Inference Server","date":"01.07.2024","tags":[{"name":"AI","slug":"ai"},{"name":"MLOps","slug":"ml-ops"}],"description":null,"canonicalUrl":null,"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"slug\": \"nvidia-triton-inference-server\",\n  \"title\": \"Technical Notes: NVIDIA Triton Inference Server\",\n  \"date\": \"2024-07-01T00:00:00.000Z\",\n  \"tags\": [\"AI\", \"MLOps\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Recently, I explored the NVIDIA Triton Inference Server, and it completely changed how I think about deploying and optimizing AI models at scale. Packed with robust features, including multi-framework support, dynamic batching, and inference acceleration, Triton is a must-know tool for engineers solving real-world challenges in AI inference. From managing complex pipelines to fine-tuning configurations for maximum throughput, Triton offers a wealth of possibilities. Here are my key learnings and technical notes, which I hope will serve as a quick reference for anyone diving into Triton.\"), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"691px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"126.66666666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAIAAAC+dZmEAAAACXBIWXMAAAsTAAALEwEAmpwYAAADDklEQVQ4y4WU227bRhCG83i96NP0HdqbAn2BAj1cBL127noRFC3gFrHqKFZk6ixHErncE8k9zXKX5HILSo3ryK4zIBaYBT/8MzuD/0X8NPpThOELxzOG4SI+FS8eJt77EELjG2HyCpDzNTglIK+dDaF/Dg4hzGaz3W5XW5ep6538y3iqHEJ6bJ0MXf8Z5b7vu66z4BgtMM4pJZSwgosafAjd5+AYYowC8NX8YnH3rmBVshldzS5QOQeolZYGtDGmrusnlQd4L66+//uLVfVrjDFhFz9cf3mV/sgrvNnf3k7n0+k0SRIAeAgPLdm2gJa6vqrKoVStVQ2es8oFIQBLm7sg7mfyH3xK7vhonFxOFm8QPjRNizF2zjWtM56u75J9vsz15KjTn5U95MxswFgLzhjw3iOEAKCunfdtkgyD4LA5h4+yQ85hHWNvrRVCtG3LGHO1K8uyrr00bJ8tsFw+odz/q7xinCKEsiwriqLruqIoKKX5MRinWZXc9zjAfd/Xde2c6/tBmTKCEDocDpTSk/jhcNjv95zzSpTnsPd+NpstFosc4VPZAKCUOu1cjBEArLUxRq1lWt6eKzfH6NrAzBpjNB6PR6MRpbRpGmvtzc3NaDQyBqQS2Rn8ac9D2TnKMcac86ZpvPeEkOHlnBOyeg4meskLUglelqUxJsaolHLOTadTAGuMSp+BqV6VgislKKUY465r0zSVUqZpaq1VWqbF9DmYl7gUvKyKNE3Lqtxut0IIADDGME6xnP/vnKleloJKWVFGOOcxxpOy1hoAutCeNiwczeWJB6tkAWC0VlIM05JKn0YVuj70Hbfr+10+322iZ6FvDChMMCaIMkJYXpSciM2GXVJxR8p9Viav5l9NycUjZVim8hqLxRJdHtj7Nf7zwKcb+obo+ZL8vuPXXO2kJe/yXzI5ftSzWf62+Rapyevt17mevN5+g9Tkjw/fZdX75GZNCX341OdwAdu36UtmVqPDTxxWo/RnapZvs5dUrz7c7YWsPvpAf/r/zMMGC7z3o49nF2MfQnjs3v8AQIWN0BIEv8IAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"image 0\",\n    \"title\": \"image 0\",\n    \"src\": \"/static/2ab8683eed623ace5ddb3cf3b71669da/091b0/image_0.png\",\n    \"srcSet\": [\"/static/2ab8683eed623ace5ddb3cf3b71669da/5243c/image_0.png 240w\", \"/static/2ab8683eed623ace5ddb3cf3b71669da/ab158/image_0.png 480w\", \"/static/2ab8683eed623ace5ddb3cf3b71669da/091b0/image_0.png 691w\"],\n    \"sizes\": \"(max-width: 691px) 100vw, 691px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(\"h2\", null, mdx(\"strong\", {\n    parentName: \"h2\"\n  }, \"Problem Overview\")), mdx(\"p\", null, \"AI inference solutions need to address:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Model Management\"), \": Multiple frameworks, devices, and versions.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Dynamic Model Handling\"), \": Loading/unloading models without disrupting live services.\")), mdx(\"hr\", null), mdx(\"h3\", null, mdx(\"strong\", {\n    parentName: \"h3\"\n  }, \"1. Model Deployment\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Multi-Framework Support\"), \": Deploy PyTorch, TensorFlow, and ONNX models on the same server.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Hardware Flexibility\"), \": Assign different models to GPUs, CPUs, or specific devices.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Version Management\"), \": By default, Triton serves the latest model version, but this behavior is configurable.\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Model Control Modes\"), \":\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"NONE\"), \": Static configuration, no dynamic updates.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"EXPLICIT\"), \": Manual API-driven updates.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"POLL\"), \": Automatic polling for repository updates.\")), mdx(\"p\", null, \"Example:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"tritonserver --model-repository=/models --model-control-mode=poll\\n\")), mdx(\"hr\", null), mdx(\"h3\", null, mdx(\"strong\", {\n    parentName: \"h3\"\n  }, \"2. Concurrent Execution & Dynamic Batching\")), mdx(\"h4\", null, mdx(\"strong\", {\n    parentName: \"h4\"\n  }, \"Dynamic Batching\"), \":\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Combines multiple inference requests dynamically into a single batch.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Configurable in \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"config.pbtxt\"), \":\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-plaintext\"\n  }, \"dynamic_batching {\\n    max_queue_delay_microseconds: 100\\n}\\n\")), mdx(\"h4\", null, mdx(\"strong\", {\n    parentName: \"h4\"\n  }, \"Concurrent Execution\"), \":\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Multiple Instances\"), \": Spawn multiple instances of the same model for parallel processing.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Instance Placement\"), \": Allocate instances across GPUs for optimal performance.\")), mdx(\"p\", null, \"Example:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-plaintext\"\n  }, \"instance_group [\\n  {\\n    count: 2\\n    kind: KIND_GPU\\n    gpus: [ 0, 1 ]\\n  }\\n]\\n\")), mdx(\"hr\", null), mdx(\"h3\", null, mdx(\"strong\", {\n    parentName: \"h3\"\n  }, \"3. Optimizing Triton Configuration\")), mdx(\"p\", null, \"Every inference workload is unique. Triton allows configuration sweeps to balance:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Network Latency\"), \": Reduce transfer overhead, especially for large datasets.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Compute Latency\"), \": Accelerate with optimizations like FP16 precision, fused kernels, etc.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Queue Latency\"), \": Address delays by increasing model instances.\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Model Analyzer\"), \":\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A CLI tool for configuration sweeps and performance profiling.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Tracks latency, throughput, GPU utilization, and more.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Outputs detailed reports and metrics to guide deployments.\")), mdx(\"hr\", null), mdx(\"h3\", null, mdx(\"strong\", {\n    parentName: \"h3\"\n  }, \"4. Inference Acceleration\")), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"960px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"60.83333333333334%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAACU0lEQVQoz2WRu09TcRTH+R90UycGV8Og0RgHBxMUHIzBSRdXHEAe4sTiUB1AE0sgVsFHYwADBaVYHoIEKQVKL0WlIK21EGqhtLe3997f+2tuHfDxzZnOySff7zmnjBGZ0VhG4xmNW0WqIBRXANLG6kZ2PLb3PmcnJZiUHMBaaskfafcvty+sjwIo03fUQCWGryjfJSQjeatoEV1BYCx2zx06+3j+tLbVbxi6kbHB4Zt51DJ06M7A4c7ATQfmtq6WPIj0IOyh+e/UYqJoQyK89cK/fnd47XY8OyUZJYUiGPyz7vqXx+q6j3pG6xyYFXbt3gbS30R7G/XtCLVzolCEwmyi7XX0ule79iXts+wMM3+CYGyxwzV+4n6g4tVMgwMLpZwVSxKAYrS4r1MuOJOCQ3BwJhmldi4vKBiHlVdmXlEbyoktpeSUEYtaJmEsu78f39xklOJvSSkAfE7O9YUaB8MtH6LPSs7SMRbCmXEpdV1PpX5wqjb2JqYTD6birs3sjEn0nL4LYCzc09x/pPVteUfghgNbhGiaFgwGNU3Ti7qQzKYmgI+Jds9S5ZPFCwupHosY2XwGwGS4r667vNl7vHOk1oG3dnYuVldframpqrocinwyrBwl/2YuxXb+PL82+HDiZNfsuTfBJiiUyYN7QQKUccM0hRCM0VIRxinjjFAGwDuxUlHvO9MyUuueLDkbpjsae/413hGNfcsXADDOxX+izHH2TmunWofOuwK3npbgAqFz2+nl9G5wO521CYA/fneg3813iaRrbr4ttNC1smoL+QsYcWmLaXK1GQAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"image 1\",\n    \"title\": \"image 1\",\n    \"src\": \"/static/5f46563398f66c2284c8dbaf9151228a/7d769/image_1.png\",\n    \"srcSet\": [\"/static/5f46563398f66c2284c8dbaf9151228a/5243c/image_1.png 240w\", \"/static/5f46563398f66c2284c8dbaf9151228a/ab158/image_1.png 480w\", \"/static/5f46563398f66c2284c8dbaf9151228a/7d769/image_1.png 960w\", \"/static/5f46563398f66c2284c8dbaf9151228a/87339/image_1.png 1440w\", \"/static/5f46563398f66c2284c8dbaf9151228a/c24c2/image_1.png 1703w\"],\n    \"sizes\": \"(max-width: 960px) 100vw, 960px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \")), mdx(\"h4\", null, mdx(\"strong\", {\n    parentName: \"h4\"\n  }, \"Backend Support\"), \":\"), mdx(\"p\", null, \"Triton supports various backends:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"GPU\"), \": TensorRT, CUDA Execution Provider.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"CPU\"), \": OpenVINO for optimized performance.\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TensorRT\"), \":\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Integrates with PyTorch and TensorFlow (\", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"Torch-TensorRT\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"TF-TensorRT\"), \") for fallback handling.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Example config for FP16 acceleration:\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-plaintext\"\n  }, \"optimization {\\n  execution_accelerators {\\n    gpu_execution_accelerator : [ {\\n      name : \\\"tensorrt\\\"\\n      parameters { key: \\\"precision_mode\\\" value: \\\"FP16\\\" }\\n    }]\\n  }\\n}\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CPU Acceleration\"), \":\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Use OpenVINO for efficient inference on CPUs.\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-plaintext\"\n  }, \"optimization {\\n  execution_accelerators {\\n    cpu_execution_accelerator : [ { name : \\\"openvino\\\" } ]\\n  }\\n}\\n\")), mdx(\"hr\", null), mdx(\"h3\", null, mdx(\"strong\", {\n    parentName: \"h3\"\n  }, \"5. Model Ensembles\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Execute multiple models in a single \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Directed Acyclic Graph (DAG)\"), \" pipeline with one network call.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Reduces client-server data transfer and latency.\")), mdx(\"p\", null, \"For complex logic (loops, conditionals), use the Python or C++ backend with Triton\\u2019s \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Business Logic Scripting API (BLS)\"), \".\"), mdx(\"hr\", null), mdx(\"h3\", null, mdx(\"strong\", {\n    parentName: \"h3\"\n  }, \"6. Building Complex Pipelines\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Multiple contributors can integrate their work seamlessly using the Python or C++ backend.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Ideal for collaborative projects with modular deep learning pipelines.\")), mdx(\"hr\", null), mdx(\"h3\", null, mdx(\"strong\", {\n    parentName: \"h3\"\n  }, \"Performance Optimization Summary\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Dynamic Batching\"), \": Improves throughput/latency by grouping requests.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Parallel Model Instances\"), \": Reduces wait time in model queues.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Accelerator Integration\"), \": GPU-based TensorRT and CPU-based OpenVINO.\")), mdx(\"hr\", null), mdx(\"h3\", null, \"Key Takeaways:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Combine multiple inference requests into a single batch for better speed and resource utilization. Configurable in config.pbtxt.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Run multiple instances of the same model on one or more GPUs to improve throughput and reduce queue latency.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Run multiple models in sequence (pipeline) with a single network request to minimize latency.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Use the Model Analyzer CLI to identify optimal configurations for latency, throughput, and resource usage.\")), mdx(\"p\", null, \"Tip: Use the NGC PyTorch container with Docker as your environment. It saves time, avoids setup headaches, and provides a pre-configured, optimized setup for development and deployment.\"), mdx(\"p\", null, \"Ref:\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide\"\n  }, \"https://github.com/triton-inference-server/tutorials/tree/main/Conceptual_Guide\")));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"Recently, I explored the NVIDIA Triton Inference Server, and it completely changed how I think about deploying and optimizing AI models at…","timeToRead":2,"banner":null}},"pageContext":{"slug":"/nvidia-triton-inference-server","formatString":"DD.MM.YYYY"}},
    "staticQueryHashes": ["2744905544","3090400250","318001574"]}